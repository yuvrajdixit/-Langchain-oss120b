from transformers import pipeline
from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, ConversationChain
from langchain.memory import ConversationBufferMemory

# ğŸ”¹ Load the OSS 120B Model from Hugging Face using pipeline
print("â³ Loading GPT-OSS-120B model...")
pipe = pipeline(
    "text-generation",
    model="openai/gpt-oss-120b",
    torch_dtype="auto",
    device_map="auto",
    max_new_tokens=256,
)
print("âœ… Model loaded successfully.")

# ğŸ”¹ Wrap the pipeline in LangChain's HuggingFacePipeline LLM
llm = HuggingFacePipeline(pipeline=pipe)

# ğŸ”¹ Example 1: Using LLMChain with a PromptTemplate
prompt = PromptTemplate(
    input_variables=["topic"],
    template="Explain {topic} clearly and concisely.",
)

chain = LLMChain(llm=llm, prompt=prompt)

print("\nğŸ”¸ Running prompt chain:")
response = chain.run("quantum mechanics")
print("ğŸ§  Response:\n", response)

# ğŸ”¹ Example 2: Using Conversational Memory
memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory, verbose=True)

print("\nğŸ”¸ Running conversational memory chain:")
conversation.predict(input="Hello, who are you?")
final_response = conversation.predict(input="Explain black holes like I'm 5.")
print("ğŸ§  Final Response:\n", final_response)
